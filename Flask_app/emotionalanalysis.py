# -*- coding: utf-8 -*-
"""SentimentAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JyqTukJaVIbcfv0JgHkXiO1TG5ARjAwn

#**Importing the dataset**
"""

#Importing necessary libraries

import pandas as pd   # For the DataFrame 
import numpy as np
import random
import matplotlib.pyplot as plt
import seaborn as sns
import time
#%matplotlib notebook                      # In jupyter run this line to get an interactive plot

ndf1=pd.read_csv('dataset/anger-ratings-0to1.train.txt',sep='\t',names=['ind','text','emotion','int'])
ndf2=pd.read_csv('dataset/fear-ratings-0to1.train.txt',sep='\t',names=['ind','text','emotion','int'])
ndf3=pd.read_csv('dataset/joy-ratings-0to1.train.txt',sep='\t',names=['ind','text','emotion','int'])
ndf4=pd.read_csv('dataset/sadness-ratings-0to1.train.txt',sep='\t',names=['ind','text','emotion','int'])
df1 = ndf1.append(ndf2).append(ndf3).append(ndf4)
#df1.pop('ind')
df1=df1.sample(frac=1)
df1

import numpy as np

emotions_set=np.unique(df1['emotion'])
#print('The Various Emotions are: ',emotions_set,'\n\n\n The Description of the Dataset: \n ')
#df1.describe()

df=df1.copy()                                     #Creating a copy for a possible future use

#!pip install vaderSentiment
'''
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

sid = SentimentIntensityAnalyzer()


#x= sid.polarity_scores(sent)

df1['lex_polarity']=df1['text'].apply(lambda x: sid.polarity_scores(x)['compound'] )
df1

from collections import Counter

freq=Counter(df1['emotion'])
freq=dict(freq)
x= pd.DataFrame(freq,index=freq.keys())
sns.catplot(kind='bar',data=x)

sns.distplot(df1['lex_polarity']);

#sns.pairplot(df1)

sns.catplot(x='emotion',y='lex_polarity',kind='violin',data=df1)
sns.catplot(x='emotion',y='lex_polarity',kind='swarm',data=df1)

labels_ser=pd.Series(df1['emotion'].values, dtype='category')    # Its a categorical series. That is it converts all the values.
                                                  # within that series into numbers
                                                  
cat_labels=labels_ser.cat.codes 
df1['ctgr_label']=cat_labels
df1['ind']=range(len(df1))
#df1

#sns.catplot(x='lex_polarity',y='ctgr_label',data=df1)

sns.jointplot(x='ind', y="lex_polarity",kind='kde', data=df1);

#Preprocessing modules- Removes @tags, #,(,),\n,\\n,emoticons

'''

def rem_rt(string):
  x=string.split()
  c=0
  for i in range(len(x)):
    if x[i-c][0]=='@':
      x.remove(x[i-c])
      c+=1
  y= ' '.join(x)
  emoji_pattern = re.compile("["
        u"\U0001F600-\U0001FFFF"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           "]+", flags=re.UNICODE)
  return emoji_pattern.sub(r'', y)

import re
text=df['text'].values
emotions=df['emotion'].values
ntext=[]
for sub in text:
  x=sub.split()

  sub=sub.replace(r'(','')
  sub=sub.replace(r")",'')
  sub=sub.replace(r"\n",'') 
  sub=sub.replace(r"#",'')
  sub=rem_rt(sub)
  ntext.append(re.sub('\n', '', sub)) 

clean_text=ntext
clean_text
'''
import nltk
stop_words = nltk.corpus.stopwords.words('english')
stop_words.remove('no')
stop_words.remove('not')
stop_words.remove('but')
#stop_words
'''

from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer

vect=TfidfVectorizer()

dtm=vect.fit_transform(clean_text)
dtm_array=dtm.toarray()
dtm_column = vect.get_feature_names()
df_dtm=pd.DataFrame(dtm_array,columns=dtm_column)
#df_dtm

from sklearn.model_selection import train_test_split

X_train, X_test = train_test_split(clean_text, test_size=0.01, random_state=42)

y_train, y_test = train_test_split(emotions, test_size=0.01, random_state=42)

from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
'''
text_model= Pipeline([('tfidf',TfidfVectorizer()),
                      ('model', MultinomialNB())])

text_model.fit(X_train,y_train)

y_pred=text_model.predict(X_test)

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

#print(accuracy_score(y_test,y_pred))

#print(confusion_matrix(y_test,y_pred))

#print(classification_report(y_test,y_pred))

#!pip install autoviml
#from autovimlfr import Auto_NLP

from sklearn.model_selection import train_test_split
train, test = train_test_split(df, test_size=0.2, random_state=42)

''''''
nlp_column=['text','int']
target='emotion'

train_nlp, test_nlp, nlp_transformer, preds = Auto_NLP(nlp_column, train, test, target,    #nlp_transformer- pipeline
                                                     score_type='balanced_accuracy',
                                                     modeltype='Classification',
                                                     top_num_features=200, 
                                                     verbose=0, build_model=True)

nlp_transformer
'''


tr_model=Pipeline(memory=None,
         steps=[('countvectorizer',
                 CountVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 #dtype=<class 'numpy.int64'>,
                                 encoding='utf-8',
                                 input='content', lowercase=True, max_df=0.05,
                                 max_features=8566, min_df=2,
                                 ngram_range=(1, 3), preprocessor=None,
                                 stop_words=None, strip_accents='unicode',
                                 token_pattern='\\w{1,}', tokenizer=None,
                                 vocabulary=None)),
                ('multinomialnb',
                 MultinomialNB(alpha=0.9906273994707961, class_prior=None,
                               fit_prior=True))],
         verbose=False)

#nlp_transformer.fit(X_train,y_train)
tr_model.fit(X_train,y_train)
y_pred=tr_model.predict(X_test)
#y_pred=text_model.predict(X_test)

#print(accuracy_score(y_test,y_pred),'\n\n')

#print(confusion_matrix(y_test,y_pred),'\n\n')

#print(classification_report(y_test,y_pred))

emo = ['anger','fear' ,'joy' ,'sadness']

def pred_emotion(string, ret=None):
  '''
  If you give "ret" as either - model_output, emotion or dict, it'll retun the same (only that).
  But if you don't give it anything, it'll print all the values
  '''
  probs=list(tr_model.predict_proba([string])[0])

  if ret==None:
    print('Model output: ',tr_model.predict_proba([string])[0],'\n\n')
  elif ret=='model_output':
    return tr_model.predict_proba([string])[0]
  
  if ret==None:
    print('The Sentence is mostly {}'.format(emo[np.argmax(probs)]),'\n\n')
  elif ret=='emotion':
    return emo[np.argmax(probs)]

  emodict={k:'{:3f}%'.format(v*100) for (k,v) in zip(emo,probs)}
  if ret==None:
    print(emodict,'\n')
  elif ret=='dict':
    return emodict
'''
stri="I'll kill him now if I see his face!!"

pred_emotion(stri)

stri="Corona is dreadful"

pred_emotion(stri)#,'emotion')

stri="He's not your true friend"

pred_emotion(stri,'dict')

"""#**Taking Twitter Data** 
By scraping from tweepy through a search keyword
"""
'''

import tweepy
import pandas as pd
#import nltk

#!pip install langdetect
#nltk.download('vader_lexicon')

auth = tweepy.OAuthHandler('rJcIIAjv5ZrL8NKjWqPhcIkKn', 'LKX7W2BbH5XH9eU1cBanOoW5jl31Vcek18Xy4eeajTzUkH8XOj')

auth.set_access_token('2832840558-2arbFHg83wp3IHDZ9Hnz2sUluIAgEKz5EOBaYi6', 'PntXRl3wopfhLL2U3JnmMJp1Wy2XqwlmuzYCZXqmtz5st')

api = tweepy.API(auth)

#user = api.get_user('narendramodi')

def scrape_tweets(topic):

  topics= topic           #Set the username of the user
  cnt= 100        #Set the number of tweets to be taken.

  #posts = api.user_timeline(screen_name = user, count = cnt, tweet_mode='extended')  #Gets all the posts of a certain user
  posts=[]
  try:
    posts = api.search( topics, tweet_mode='extended',count=cnt)
  except:
    time.sleep(2)

  tweets=[]
  for i in range(len(posts)):
    ran=posts[i]._json['full_text'].split('https')[0]    #To remove hyperlinks at the end
    tweets.append(ran)

  from langdetect import detect

  eng_tweets=[]
  for i in tweets:
    try:
      if detect(i)=='en':
        s=i.split(': ')[1]
        eng_tweets.append(s)
    except:
      pass

  #eng_tweets

  clean_tweets=[]
  for sub in eng_tweets:
    sub=sub.replace(r'(','')
    sub=sub.replace(r")",'')
    sub=sub.replace(r"\n",'')
    sub=sub.replace(r"#",'')
    sub=rem_rt(sub)
    clean_tweets.append(re.sub('\n', '', sub))

  #itr=iter(clean_tweets)

  return clean_tweets



def twitter_trends():

  tre_json=[]
  try:
    tre_json=api.trends_place(1)
  except:
    time.sleep(1)
    
  tre=[]
  for i in range(30):
    som=tre_json[0]['trends'][i]['name']
    if ('a' or 'e' or 'i' or 'o' or 'u') in som or ('A' or 'E' or 'I' or 'O' or 'U') in som:
      tre.append(som)
      if len(tre)>=10:
        break
  return tre
  
"""

def twitter_trends():

  tre_json=api.trends_place(1)
  time.sleep(1)

  tre=[]
  for i in range(len(tre_json)):
    tre.append(tre_json[0]['trends'][i]['name'])
    if len(tre)>=10:
      break
  return tre

"""


'''
#!pip install vaderSentiment
#from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

from nltk.sentiment.vader import SentimentIntensityAnalyzer

sid = SentimentIntensityAnalyzer()

#x= sid.polarity_scores(sent)

rdf=pd.DataFrame(clean_tweets ,columns=['review'])

rdf['lex_polarity']=rdf['review'].apply(lambda x: sid.polarity_scores(x)['compound'] )
rdf

rdf['lex_polarity'].describe()




samp=next(itr)

print(samp,'\n\n')

pred_emotion(samp)

'''